---
title: "HW2"
output: html_document
date: "2023-12-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE}
library(fastDummies)
library(data.table)
library(kknn)
library(dplyr)
library(rpart)
library(caret)
library(mlr)
library(rattle)
library(ranger)
library(gbm)
setwd("~/Desktop/sa")
set.seed(456)
```

# Introducing the Datasets:

We were asked to find 5 datasets with the following conditions:

-   Separate labeled test data\
-   Number of training and test samples are larger than 200\
-   More than 20 features\
-   At least one of them should be a regression problem (Student Data)  
-   At least two of them should be multi-class classification problem (Mobile Price Data, Human Activity Recognition Using Smartphones Data) \
-   At least one of them should have a class imbalance problem  (Credit Card Data)\
-   At least two of them should have more than 100 features (Musk Data, Human Activity Recognition Using Smartphones Data) \
-   At least one of them should have some number of categorical or ordinal features (Student Data, Mobile Price Data)

## Dataset 1: Student Data

This dataset aims to predict the performances of the students on the final exam of a Portuguese course. The data has 32 features, one of them is the target, and is normally a regression problem. Features are both nominal (like age, number of school absences, previous grades etc.) and categorical (like student's guardian, father's job, mother's job etc.). There are 400 samples in the train data and 200 labeled samples in test data.

```{r,include=FALSE,echo=FALSE}
#Student Data
student=read.table("student-por.csv",sep=";",header=TRUE)
# Convert binary variables to 0 and 1
student$school <- ifelse(student$school == "GP", 0, 1)
student$sex <- ifelse(student$sex == "F", 0, 1)
student$address <- ifelse(student$address == "U", 0, 1)
student$famsize <- ifelse(student$famsize == "LE3", 0, 1)
student$Pstatus <- ifelse(student$Pstatus == "T", 0, 1)
student$schoolsup <- ifelse(student$schoolsup == "no", 0, 1)
student$famsup <- ifelse(student$famsup == "no", 0, 1)
student$paid <- ifelse(student$paid == "no", 0, 1)
student$activities <- ifelse(student$activities == "no", 0, 1)
student$nursery <- ifelse(student$nursery == "no", 0, 1)
student$higher <- ifelse(student$higher == "no", 0, 1)
student$internet <- ifelse(student$internet == "no", 0, 1)
student$romantic <- ifelse(student$romantic == "no", 0, 1)


student$target <- student$G3
student$G3 <- NULL  # Remove the old column.

# Convert some columns to factors
student[, sapply(student, is.character)] <- lapply(student[, sapply(student, is.character)], as.factor)
student_train <- student[1:449,]
student_test <- student[450:649,]
rownames(student_test) <- NULL
```

After cleaning the data by turning characterical columns into categorical ones, here is the head of the data:

```{r}
head(student,5)
```

```{r,echo=FALSE, include=FALSE}
#Specific dataset for Nearest Neighbor since it can't handle categorical data.
student_nn <- copy(student)
# Specify the columns to be transformed to be used in k-NN
categorical_columns <- c("Mjob", "Fjob", "reason", "guardian")
# Create dummy variables using one-hot encoding
student_nn <- dummy_cols(student_nn, select_columns = categorical_columns)
# Remove the original categorical columns
student_nn <- student_nn[, !(names(student_nn) %in% categorical_columns)]
# Standardize the features excluding the target variable.
student_nn[, !names(student_nn) %in% c("target")] <- scale(student_nn[, !names(student_nn) %in% c("target")])
#Turn the regression data into classification data:
student_nn$target<- as.factor(ifelse(student_nn$target >= mean(student_nn$target),"high","low"))
#Split data into train and test subsets so that we have 200 test data. (More train data would not hurt so it is okay that it is not exactly 200.)
student_nn_train <- student_nn[1:449,]
student_nn_test <- student_nn[450:649,]
rownames(student_nn_test) <- NULL
```

Since, k-Nearest Neighbor cannot handle non-numerical features I created another data called student_nn and used one-hot encoding to fix this issue. I also standardized all the features except for the target. Input data for the NN is now:

```{r}
head(student_nn,5)
```

## Dataset 2: Credit Card Data

This dataset contains transactions made by credit cards in September 2013 by European cardholders and aims to detect fraudulent credit card transactions.It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features and more background information about the data are not provided. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction amount. Feature 'Class' is the target variable and it takes value 1 in case of fraud and 0 otherwise. There are 284807 rows and 31 columns and in the feature called "Class" there is high class imbalance.

```{r,include=FALSE,echo=FALSE}
card=read.csv("creditcard.csv",header=TRUE)
card$target <- card$Class
card$Class <- NULL
```

The magnitude of the class imbalance can be seen below:
```{r}
head(card,5)
barplot(table(card$target),col="pink")
```


```{r, include=FALSE}
set.seed(123)
splitIndex <- createDataPartition(card$target, p = 0.8, list = FALSE, times = 1)
card_train <- card[splitIndex, ]
card_test <- card[-splitIndex, ]
rownames(card_train) <- NULL
rownames(card_test) <- NULL

# Initialize empty data frames to store sampled data
card_train_sampled <- data.frame()
card_test_sampled <- data.frame()

# Sample within each class for training data
for (class_label in unique(card_train$target)) {
  class_data <- subset(card_train, target == class_label)
  sampled_rows <- class_data[sample(nrow(class_data), 5000 * (sum(card$target==class_label)/nrow(card)),replace=TRUE), ]
  card_train_sampled <- rbind(card_train_sampled, sampled_rows)
}

# Sample within each class for testing data
for (class_label in unique(card_test$target)) {
  class_data <- subset(card_test, target == class_label)
  sampled_rows <- class_data[sample(nrow(class_data), 2000 * (sum(card$target==class_label)/nrow(card)),replace=TRUE), ]
  card_test_sampled <- rbind(card_test_sampled, sampled_rows)
}

rownames(card_train_sampled) <- NULL
rownames(card_test_sampled) <- NULL


train_class_percentages <- prop.table(table(card_train_sampled$target == 1)) * 100
test_class_percentages <- prop.table(table(card_test_sampled$target == 1)) * 100
result_table <- data.frame(
  Dataset = c("Train Data","Test Data"),
  `Class=1 Ratio` = c(train_class_percentages[2], test_class_percentages[2])
)
```

Since there are more rows than my computer can handle, I created a train data of 5000 observations and test data of 2000 observations. Since there is class imbalance I made sure that each subdata's ratio of target being 1 is equal to the original data which is 0.172: 
```{r}
result_table
```

```{r, include=FALSE, echo=FALSE}
#Preprocessing for k-NN.
card_nn_train <- copy(card_train_sampled)
card_nn_train[, !names(card_nn_train) %in% c("target")] <- scale(card_nn_train[, !names(card_nn_train) %in% c("target")])
card_nn_test <- copy(card_test_sampled)
card_nn_test[, !names(card_nn_test) %in% c("target")] <- scale(card_nn_test[, !names(card_nn_test) %in% c("target")])
rownames(card_nn_test) <- NULL
rownames(card_nn_train) <- NULL

card_train_sampled$target <- as.factor(card_train_sampled$target+1)
card_test_sampled$target <- as.factor(card_test_sampled$target+1)
```

Since, we will be performing k-Nearest Neighbor, I standardized all the features except for the target. Input data for the NN is now:
```{r}
head(card_nn_train,5)
```


## Dataset 3: Mobile Price Data

This dataset aims to classify mobile phones with respect to their prices in order to predict new mobiles' price ranges with respect to their features. There are 4 categories in target: low cost, medium cost, high cost, very high cost. Including the target data has 21 features ranging from battery_power,ram, four_g, wifi to n_cores. There are both numerical and binary variables. For example battery power is numerical but four_g is a binary variable indicating whether a phone has four_g or not. There are 1800 samples for train data and 200 for the test data. 

```{r, include=FALSE}
mobile <- read.csv("train.csv",header=TRUE)
mobile$target <- as.factor(mobile$price_range+1)
mobile$price_range <- NULL
mobile_train <- mobile[1:1800,]
mobile_test <- mobile[1801:2000,]
rownames(mobile_test) <- NULL
```
This is the head of the data:
```{r}
head(mobile,5)
```

Since, we will be performing k-Nearest Neighbor, I standardized all the features except for the target. Input data for the NN is now:
```{r, include=FALSE,echo=FALSE}
mobile_nn_train <- copy(mobile_train)
mobile_nn_train[, !names(mobile_nn_train) %in% c("target")] <- scale(mobile_nn_train[, !names(mobile_nn_train) %in% c("target")])
mobile_nn_test <- copy(mobile_test)
mobile_nn_test[, !names(mobile_nn_test) %in% c("target")] <- scale(mobile_nn_test[, !names(mobile_nn_test) %in% c("target")])
```

```{r}
head(mobile_nn_train,5)
```


## Dataset 4: Musk Data

This dataset describes a set of 92 molecules of which 47 are judged by human experts to be musks and the remaining 45 molecules are judged to be non-musks.  The goal is to learn to predict whether new molecules will be musks or non-musks.  However, the 166 features that describe these molecules depend upon the exact shape, or conformation, of the molecule.  Because bonds can rotate, a single molecule can adopt many different shapes.  To generate this dataset, the low-energy conformations of the molecules were generated and then filtered to remove highly similar conformations. This left 476 conformations.  Then, a feature vector was extracted that describes each conformation.At the end, there are 169 categorical or numerical features in this data. Most of the columns are presenting some sort of chemical characteristics of the molecules. Here is the head of the data: 

```{r,include=FALSE,echo=FALSE}
musk <- read.table("clean1.data",sep=",")
clean1_names <- read.table("clean1.names", header = FALSE,sep=":")
colnames(musk) <- clean1_names$V1
musk$molecule_name <- as.factor(musk$molecule_name)
musk$conformation_name <- as.factor(musk$conformation_name)
musk$target <- as.factor(musk$Class+1)
musk$Class <- NULL
musk <- musk[sample(nrow(musk)),]
musk_train <- musk[1:276,]
musk_test <- musk[277:476,]
rownames(musk_test) <- NULL
```
```{r}
head(musk,5)
```

Since, k-Nearest Neighbor cannot handle non-numerical features I created another data called musk_nn and used one-hot encoding to fix this issue. I also standardized all the features except for the target. Input data for the NN is now:
```{r, include=FALSE, echo=FALSE}
#Specific dataset for Nearest Neighbor since it can't handle categorical data.
musk_nn <- copy(musk)
# Specify the columns to be transformed to be used in k-NN
categorical_columns <- c("molecule_name", "conformation_name")
# Create dummy variables using one-hot encoding
musk_nn <- dummy_cols(musk_nn, select_columns = categorical_columns)
# Remove the original categorical columns
musk_nn <- musk_nn[, !(names(musk_nn) %in% categorical_columns)]
# Standardize the features excluding the target variable.
musk_nn[, !names(musk_nn) %in% c("target")] <- scale(musk_nn[, !names(musk_nn) %in% c("target")])
#Split data into train and test subsets so that we have 200 test data. (More train data would not hurt so it is okay that it is not exactly 200.)
musk_nn_train <- musk_nn[1:276,]
musk_nn_test <- musk_nn[277:476,]
rownames(musk_nn_test) <- NULL
```
```{r}
head(musk_nn_train,5)
```


## Dataset 5: Human Activity Recognition Using Smartphones Data

The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.
```{r, include=FALSE,echo=FALSE}
activity_train=read.csv("train_activity.csv",header=TRUE)
activity_test = read.csv("test_activity.csv",header=TRUE)
activity_train$target <- as.factor(activity_train$Activity)
activity_train$Activity <- NULL
activity_test$target <- as.factor(activity_test$Activity)
activity_test$Activity <- NULL
activity_train <- activity_train[sample(nrow(activity_train),3600),]
activity_test <- activity_test[sample(nrow(activity_test),1800),]
```

The features are already normalized within [-1,1] and all of them are numerical except the target. Thus, our data is ready for all the algorithms including k-NN
```{r}
head(activity_train,5)
```


# Introducing the Functions:

For each algorithm, I built flexible functions to be used with every dataset performing 10-fold cross validation with the parameters that are asked to be tuned. I unfortunately, misread the assignment and built my functions for both regression and classification tasks, this is not a big issue as we will just use them for classification tasks.

## K-Nearest Neighbor

Since the distance metric is one of the parameters to be tuned and the "caret" package only uses euclidean distance while performing cv.knn, I used another package called "kknn" for this task. In the code "d_list" corresponds to Manhattan for 1 and Euclidean for 2.

```{r, warning=FALSE,error=FALSE}
k_NN <- function(train,test,k_list) {
  #Perform 10-fold cross validation using the train data.
  d_list <- c(1,2)
  cv_values <- data.frame()
  for(k in seq_along(k_list)) {
    for(d in seq_along(d_list)) {
      fit <- cv.kknn(target~ .,train,kcv=10,k=k_list[k],distance=d_list[d])
      y <- fit[[1]][,1]
      y_hat <- fit[[1]][,2]
      misclassification_error <- mean(y_hat != y)
      cv_values[k,d] <- misclassification_error
    }
  }
  #Find missclassification errors on the train set to choose the best model:
  min_value <- min(cv_values)
  min_indices <- which(cv_values == min_value, arr.ind = TRUE)[1,]
  best_k <- k_list[min_indices[["row"]]]
  best_d <- d_list[min_indices[["col"]]]
  if(best_d ==1) {
    cat("The chosen parameters are: Distance method= Manhattan, k=",best_k,"\n")
  }else{
    cat("The chosen parameters are: Distance method= Euclidean, k=",best_k,"\n")
  }
  #Train the final model and use test data to create confusion matrix. 
  final_model <- kknn(target~ .,train,test,k=best_k,distance=best_d)
  confusion_matrix <- table(test$target, final_model$fitted.values)
  acc <- sum(diag(confusion_matrix))/sum(confusion_matrix)
  cat("Accuracy on test data: ",acc,"\n")
  return(cv_values)
}
```

## Decision Trees

There were again some problems with the parameters to be tuned and what "caret" takes as parameters so I used a package called "mlr" which is pretty flexible in terms of building tasks and learners. In this function, there is also built-in factorization of the target feature if it not already prepared, however it can only be used in binary classification. 

```{r,warning=FALSE,error=FALSE}
CART <- function(train,test,type) {
  if(type=="r") {
    task <- makeRegrTask(data = train, target = "target")
    learner <- makeLearner("regr.rpart", cp = 0)
  } else {
    if(is.factor(train$target)== FALSE) {
      mean <- mean(train$target)
      train$target<- as.factor(ifelse(train$target >= mean,"high","low"))
      test$target<- as.factor(ifelse(test$target >= mean,"high","low"))
    }
    task <- makeClassifTask(data = train, target = "target")
    learner <- makeLearner("classif.rpart", cp = 0)
  }
  # Define the search space for tuning minbucket
  param_set <- makeParamSet(makeDiscreteParam("minbucket", values=c(5,8,11,14,17)))
  #Set up the cross-validation
  inner <- makeResampleDesc("CV", iters = 10)
  # Custom learner training function to set minsplit
  configureMinSplit <- function(learner, params) {
    minsplit_val <- 2 * params$minbucket
    learner$par.vals <- c(learner$par.vals, list(minsplit = minsplit_val))
    return(learner)
  }
  res <- tuneParams(learner, task, resampling = inner, par.set = param_set, control = makeTuneControlGrid())
  # Print the best parameter configuration
  print(res)
  best_minbucket<- res[["x"]][["minbucket"]]
  
  final_model <- rpart(target ~ ., data = train, cp = 0, minbucket=best_minbucket, minsplit = 2 * best_minbucket)
  new_predictions <- predict(final_model, newdata = test)
  if(type== "r") {
    rmse <- sqrt(mean((test$target - new_predictions)^2))
    cat("Final model's RMSE value on the Test Data is: ",rmse)
  } else {
    new_predictions <- apply(new_predictions, 1, function(row) {
      ifelse(row[1] > row[2], "high", "low")
    })
    confusion_matrix <- table(test$target, new_predictions)
    acc <- sum(diag(confusion_matrix))/sum(confusion_matrix)
    cat("Accuracy on test data: ",acc,"\n")
  }
  final_model[["acc"]] <- acc
  return(final_model)
}
```

## Random Forests

Here only parameter that is tuned is "mtry".I also built this function using the package "mlr". 

```{r,warning=FALSE,error=FALSE}
RF <- function(train,test,type) {
  #Creating a regression or classification task depending on the 'target' variable:
  if(type=="r") {
    task <- makeRegrTask(data = train, target = "target")
    learner <- makeLearner(
      "regr.ranger",  
      num.trees = 500,
      min.node.size = 5
    )
    
  } else {
    if(is.factor(train$target)== FALSE) {
      mean <- mean(train$target)
      train$target<- as.factor(ifelse(train$target >= mean,"high","low"))
      test$target<- as.factor(ifelse(test$target >= mean,"high","low"))
    }
    task <- makeClassifTask(data = train, target = "target")
    learner <- makeLearner(
      "classif.ranger",  
      num.trees = 500,
      min.node.size = 5
    )
  }
  #Defining the search space for tuning mtry
  param_set <- makeParamSet(
    makeIntegerParam("mtry", lower = 1, upper = min(100,ncol(train) - 1))
  )
  #Setting up the cross-validation
  inner <- makeResampleDesc("CV", iters = 10)
  res <- tuneParams(learner, task, resampling = inner, par.set = param_set, control = makeTuneControlGrid())
  print(res)
  best_mtry <- res[["x"]][["mtry"]]
  final_model <- ranger(target ~ ., data = train, mtry= best_mtry, min.bucket=5,num.trees = 500)
  new_predictions <- predict(final_model,data = test)
  if(type=="r") {
    rmse <- sqrt(mean((test$target - new_predictions[["predictions"]])^2))
    cat("Final model's RMSE value on the Test Data is: ",rmse,"\n")
  } else {
    confusion_matrix <- table(test$target, new_predictions[["predictions"]])
    acc <- sum(diag(confusion_matrix))/sum(confusion_matrix)
    cat("Accuracy on test data: ",acc,"\n")
  }
  final_model[["acc"]] <- acc
  return(final_model)
}
```

## Gradient Boosted Trees

Since "caret" package is pretty flexible for gradient boosted trees I was able to use it in this function. I performed 10-fold cross validation and used an arbitrary value of 10 for n.minobsinnode. 

```{r}
GradBM <- function(train,test,type) {
n_folds=10
fitControl=trainControl(method = "cv",
                        number = n_folds,
                       )
gbmGrid=expand.grid(n.minobsinnode = 10,
                    n.trees = c(50,100,150,200,250), 
                    shrinkage = c(0.01,0.05,0.1,0.15,0.2),
                    interaction.depth = c(1,2,3,4,5))
if(type !="r") {
  if(is.factor(train$target)== FALSE) {
    mean <- mean(train$target)
    train$target<- as.factor(ifelse(train$target >= mean,"high","low"))
    test$target<- as.factor(ifelse(test$target >= mean,"high","low"))
}
}
                     
gbm_fit=caret::train(target ~ ., data = train, 
              method = "gbm", 
              trControl = fitControl, metric=ifelse(is.factor(train$target),'Accuracy',"RMSE"),
              tuneGrid = gbmGrid,
              verbose=F) 
best_n_trees <- gbm_fit[["bestTune"]][["n.trees"]]
best_depth <- gbm_fit[["bestTune"]][["interaction.depth"]]
best_shrinkage <- gbm_fit[["bestTune"]][["shrinkage"]]
new_predictions <- predict(gbm_fit,test)
if(type=="r") {
  rmse <- sqrt(mean((test$target - new_predictions)^2))
  cat("Final model's RMSE value on the Test Data is: ",rmse,"\n")
} else {
  confusion_matrix <- table(test$target, new_predictions)
  acc <- (sum(diag(confusion_matrix))/sum(confusion_matrix))[1]
  cat("Accuracy on test data: ",acc,"\n")
}
return(gbm_fit)
}
```

# Outputs for Each Dataset:

## Dataset 1: Student Data

### k-NN:
```{r}
k_values <- c(1,3,5,7,9)
result_nn <- k_NN(student_nn_train,student_nn_test,k_values)
plot(k_values, result_nn$V2, type = 'l', col = 'blue', xlab = '# Neighbor', ylab = 'misclassification_error',xlim=c(1,10),ylim=c(min(result_nn),max(result_nn)))
lines(k_values, result_nn$V1, col = 'red')
legend('topright', legend = c('Manhattan', 'Euclidean'), col = c('red', 'blue'), lty = 1)
```

### CART:

This time there is no pre-processing needed as this is a binary classification problem using Decision Trees.(CART function handles categorizing the target feature when it is two-class.)  

```{r,include=FALSE}
result_cart <- CART(student_train,student_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen minbucket: ",result_cart[["control"]][["minbucket"]],"\n")
cat("Accuracy on test data: ",result_cart[["acc"]],"\n")
fancyRpartPlot(result_cart)
```

### RF: 

```{r,include=FALSE}
result_rf <- RF(student_train,student_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen mtry: ",result_rf[["mtry"]],"\n")
cat("Accuracy on test data: ",result_rf[["acc"]],"\n")
result_rf[["confusion.matrix"]]
```

### GBM:

```{r,warning=FALSE,error=FALSE}
result_gbm <- GradBM(student_train,student_test,"c")
plot(result_gbm)
```

## Dataset 2: Credit Card Data

### k-NN:

```{r}
k_values <- c(1,3,5,7,9)
result_nn <- k_NN(card_nn_train,card_nn_test,k_values)
plot(k_values, result_nn$V2, type = 'l', col = 'blue', xlab = '# Neighbor', ylab = 'misclassification_error',xlim=c(1,10),ylim=c(min(result_nn),max(result_nn)))
lines(k_values, result_nn$V1, col = 'red')
legend('bottomright', legend = c('Manhattan', 'Euclidean'), col = c('red', 'blue'), lty = 1)
```

### CART:

```{r,include=FALSE}
result_cart <- CART(card_train_sampled,card_test_sampled,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen minbucket: ",result_cart[["control"]][["minbucket"]],"\n")
cat("Accuracy on test data: ",result_cart[["acc"]],"\n")
fancyRpartPlot(result_cart)
```

### RF:

```{r,include=FALSE}
result_rf <- RF(card_train_sampled,card_test_sampled,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen mtry: ",result_rf[["mtry"]],"\n")
cat("Accuracy on test data: ",result_rf[["acc"]],"\n")
result_rf[["confusion.matrix"]]
```

### GBM:

```{r,warning=FALSE,error=FALSE}
result_gbm <- GradBM(card_train_sampled,card_test_sampled,"c")
plot(result_gbm)
```


## Dataset 3: Mobile Price Data

### k-NN:

```{r}
k_values <- c(1,3,5,7,9)
result_nn <- k_NN(mobile_nn_train,mobile_nn_test,k_values)
plot(k_values, result_nn$V2, type = 'l', col = 'blue', xlab = '# Neighbor', ylab = 'misclassification_error',xlim=c(1,10),ylim=c(min(result_nn),max(result_nn)))
lines(k_values, result_nn$V1, col = 'red')
legend('bottomright', legend = c('Manhattan', 'Euclidean'), col = c('red', 'blue'), lty = 1)
```

### CART:

```{r,include=FALSE}
result_cart <- CART(mobile_train,mobile_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen minbucket: ",result_cart[["control"]][["minbucket"]],"\n")
cat("Accuracy on test data: ",result_cart[["acc"]],"\n")
fancyRpartPlot(result_cart)
```

### RF:

```{r,include=FALSE}
result_rf <- RF(mobile_train,mobile_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen mtry: ",result_rf[["mtry"]],"\n")
cat("Accuracy on test data: ",result_rf[["acc"]],"\n")
result_rf[["confusion.matrix"]]
```

### GBM:

```{r,warning=FALSE,error=FALSE}
result_gbm <- GradBM(mobile_train,mobile_test,"c")
plot(result_gbm)
```

## Dataset 4: Musk Data

### k-NN:

```{r}
k_values <- c(1,3,5,7,9)
result_nn <- k_NN(musk_nn_train,musk_nn_test,k_values)
plot(k_values, result_nn$V2, type = 'l', col = 'blue', xlab = '# Neighbor', ylab = 'misclassification_error',xlim=c(1,10),ylim=c(min(result_nn),max(result_nn)))
lines(k_values, result_nn$V1, col = 'red')
legend('bottomright', legend = c('Manhattan', 'Euclidean'), col = c('red', 'blue'), lty = 1)
```

### CART:

```{r,include=FALSE}
result_cart <- CART(musk_train,musk_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen minbucket: ",result_cart[["control"]][["minbucket"]],"\n")
cat("Accuracy on test data: ",result_cart[["acc"]],"\n")
fancyRpartPlot(result_cart)
```

### RF:

```{r,include=FALSE}
result_rf <- RF(musk_train,musk_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen mtry: ",result_rf[["mtry"]],"\n")
cat("Accuracy on test data: ",result_rf[["acc"]],"\n")
result_rf[["confusion.matrix"]]
```

### GBM:

```{r,warning=FALSE,error=FALSE}
result_gbm <- GradBM(musk_train,musk_test,"c")
plot(result_gbm)
```

## Dataset 5: Human Activity Recognition Using Smartphones Data

### k-NN:

```{r}
k_values <- c(1,3,5,7,9)
result_nn <- k_NN(activity_train,activity_test,k_values)
plot(k_values, result_nn$V2, type = 'l', col = 'blue', xlab = '# Neighbor', ylab = 'misclassification_error',xlim=c(1,10),ylim=c(min(result_nn),max(result_nn)))
lines(k_values, result_nn$V1, col = 'red')
legend('bottomright', legend = c('Manhattan', 'Euclidean'), col = c('red', 'blue'), lty = 1)
```

### CART:

```{r,include=FALSE}
result_cart <- CART(activity_train,activity_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen minbucket: ",result_cart[["control"]][["minbucket"]],"\n")
cat("Accuracy on test data: ",result_cart[["acc"]],"\n")
fancyRpartPlot(result_cart)
```

### RF:

```{r,include=FALSE}
result_rf <- RF(activity_train,activity_test,"c")
```
```{r,warning=FALSE,error=FALSE}
cat("Chosen mtry: ",result_rf[["mtry"]],"\n")
cat("Accuracy on test data: ",result_rf[["acc"]],"\n")
result_rf[["confusion.matrix"]]
```
### GBM:

```{r,warning=FALSE,error=FALSE}
result_gbm <- GradBM(activity_train,activity_test,"c")
plot(result_gbm)
```

# Findings and Discussion:

-   k-NN performed significantly worse than other methods in the student data.

-   In the credit card data all methods performed great. However, test error were still lower than validation error.

-   For the mobile price data RF and GBM performed much better than CART and k-NN. We can comment on the superiority of the former two methods on the latter two for multi-class classification.

-   In the Musk data GBM performed poorly compared to others. We can maybe say for the data with high dimensions, simpler methods are better like k-NN.

-   In the Human Activity Recognition Using Smartphones data, which is a multi-class classification using many features, CART performed poorly compared to others. However, it is worth noting that even though GBM and RF performed slightly better than k-NN, their run-times (especially of GBM) were unbearable so in many cases k-NN might be preferred over them.

# References and Datasets:

-   **Student Data:** <https://archive.ics.uci.edu/dataset/320/student+performance>

-   **Credit Card Data:** <https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data>

-   **Mobile Price Data:** <https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification>

-   **Musk Data:** <https://archive.ics.uci.edu/dataset/74/musk+version+1>

-   **Human Activity Recognition Using Smartphones Data:** <https://www.kaggle.com/datasets/uciml/human-activity-recognition-with-smartphones>

